{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":118082,"databundleVersionId":14294892,"sourceType":"competition"},{"sourceId":13781728,"sourceType":"datasetVersion","datasetId":8772412},{"sourceId":13781759,"sourceType":"datasetVersion","datasetId":8772438},{"sourceId":13781793,"sourceType":"datasetVersion","datasetId":8772466},{"sourceId":13781836,"sourceType":"datasetVersion","datasetId":8772503}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- ADD THIS NEW BLOCK FOR AUTHENTICATION ---\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\n# Access the secret\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\n# Log in to Hugging Face\nlogin(token=hf_token)\n# --- END OF NEW BLOCK ---","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device='cuda'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:01:32.028416Z","iopub.execute_input":"2025-11-17T22:01:32.028628Z","iopub.status.idle":"2025-11-17T22:01:36.062491Z","shell.execute_reply.started":"2025-11-17T22:01:32.028613Z","shell.execute_reply":"2025-11-17T22:01:36.061899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nfrom sentence_transformers import SentenceTransformer\nimport warnings\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n\n# --- Device Setup ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --- Load Data ---\nprint(\"Loading data...\")\ntrain_df = pd.read_json('/kaggle/input/da5401-2025-data-challenge/train_data.json')\ntest_df = pd.read_json('/kaggle/input/da5401-2025-data-challenge/test_data.json')\n\nwith open('/kaggle/input/da5401-2025-data-challenge/metric_names.json', 'r') as f:\n    import json\n    metric_names = json.load(f)\nmetric_to_index = {name: i for i, name in enumerate(metric_names)}\n\nmetric_embeddings = np.load('/kaggle/input/da5401-2025-data-challenge/metric_name_embeddings.npy')\n\n# --- Initialize Embedding Model ---\nprint(\"Initializing embedding model...\")\nembedding_model = SentenceTransformer(\"google/embeddinggemma-300m\").to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:01:36.309661Z","iopub.execute_input":"2025-11-17T22:01:36.309839Z","iopub.status.idle":"2025-11-17T22:02:25.146574Z","shell.execute_reply.started":"2025-11-17T22:01:36.309827Z","shell.execute_reply":"2025-11-17T22:02:25.145714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['score'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:02:25.147419Z","iopub.execute_input":"2025-11-17T22:02:25.147705Z","iopub.status.idle":"2025-11-17T22:02:25.162320Z","shell.execute_reply.started":"2025-11-17T22:02:25.147683Z","shell.execute_reply":"2025-11-17T22:02:25.161388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['score'] = train_df['score'].replace({\n    4.0: 3.0,\n    5.0: 6.0,\n    9.5: 10.0\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:02:25.163230Z","iopub.execute_input":"2025-11-17T22:02:25.163521Z","iopub.status.idle":"2025-11-17T22:02:25.566569Z","shell.execute_reply.started":"2025-11-17T22:02:25.163470Z","shell.execute_reply":"2025-11-17T22:02:25.565794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['score'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:02:25.567251Z","iopub.execute_input":"2025-11-17T22:02:25.567505Z","iopub.status.idle":"2025-11-17T22:02:25.584014Z","shell.execute_reply.started":"2025-11-17T22:02:25.567465Z","shell.execute_reply":"2025-11-17T22:02:25.583382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['score'] = train_df['score'].replace({\n    1.0: 1.5,\n    2.0: 1.5,\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:02:25.584704Z","iopub.execute_input":"2025-11-17T22:02:25.584940Z","iopub.status.idle":"2025-11-17T22:02:25.600519Z","shell.execute_reply.started":"2025-11-17T22:02:25.584923Z","shell.execute_reply":"2025-11-17T22:02:25.599799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['score'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:02:25.601280Z","iopub.execute_input":"2025-11-17T22:02:25.601804Z","iopub.status.idle":"2025-11-17T22:02:25.617201Z","shell.execute_reply.started":"2025-11-17T22:02:25.601780Z","shell.execute_reply":"2025-11-17T22:02:25.616408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset\nimport pandas as pd\n# --- Preprocessing Function ---\ndef preprocess_for_attention_model(df, is_train=True):\n    df['system_prompt'] = df['system_prompt'].fillna('')\n    df['combined_prompt'] = \"SYSTEM PROMPT: \" + df['system_prompt'] + \" USER PROMPT: \" + df['user_prompt']\n\n    print(f\"Generating embeddings for {len(df)} rows...\")\n    \n    # Encode texts\n    prompt_embeds = embedding_model.encode(df['combined_prompt'].tolist(), show_progress_bar=True, batch_size=64)\n    response_embeds = embedding_model.encode(df['response'].tolist(), show_progress_bar=True, batch_size=64)\n\n    # Move to CPU numpy\n    if isinstance(prompt_embeds, torch.Tensor): prompt_embeds = prompt_embeds.cpu().numpy()\n    if isinstance(response_embeds, torch.Tensor): response_embeds = response_embeds.cpu().numpy()\n    torch.cuda.empty_cache()\n\n    # Get Metric Embeddings\n    metric_embeds = np.array([metric_embeddings[metric_to_index[name]] for name in df['metric_name']])\n\n    data_dict = {\n        'metric_embedding': metric_embeds,\n        'prompt_embedding': prompt_embeds,\n        'response_embedding': response_embeds\n    }\n\n    if is_train:\n        targets = df['score'].astype(float).values\n        return data_dict, targets\n    else:\n        return data_dict\n\n# --- Process Data ---\nprint(\"Processing Training Data...\")\ntrain_data_dict, y_train_all = preprocess_for_attention_model(train_df, is_train=True)\nprint(\"Processing Test Data...\")\ntest_data_dict = preprocess_for_attention_model(test_df, is_train=False)\n\n\n# --- Part 3: Convert to Tensors and Perform a Stratified Split ---\n\nprint(\"\\n--- Creating Full Dataset Tensors ---\")\n# Convert all NumPy arrays to PyTorch tensors\nfull_metric_tensors = torch.tensor(train_data_dict['metric_embedding'], dtype=torch.float32)\nfull_prompt_tensors = torch.tensor(train_data_dict['prompt_embedding'], dtype=torch.float32)\nfull_response_tensors = torch.tensor(train_data_dict['response_embedding'], dtype=torch.float32)\nfull_target_tensors = torch.tensor(y_train_all, dtype=torch.float32).view(-1, 1)\n\n# Create the full TensorDataset\nfull_dataset = TensorDataset(full_metric_tensors, full_prompt_tensors, full_response_tensors, full_target_tensors)\n\n# --- NEW: Stratified Splitting ---\nprint(\"\\n--- Performing Stratified Split to Create Train/Val Datasets ---\")\n\n# Step 1: Create bins from the continuous target scores to use for stratification.\n# This ensures that each bin has a similar distribution in both train and val sets.\nnum_bins = 15 # A reasonable number of bins to capture the distribution shape\nscore_bins = pd.cut(y_train_all, bins=num_bins, labels=False)\n\n# Step 2: Generate indices for the split using sklearn's train_test_split\n# We split the indices (0 to len-1) instead of the data itself.\ndataset_indices = list(range(len(full_dataset)))\ntrain_indices, val_indices = train_test_split(\n    dataset_indices,\n    test_size=0.2,       # 80/20 split\n    stratify=score_bins, # Stratify based on our new bins\n    random_state=42\n)\n\n# Step 3: Create PyTorch Subsets from the full dataset using the stratified indices\ntrain_dataset = Subset(full_dataset, train_indices)\nval_dataset = Subset(full_dataset, val_indices)\n\nprint(f\"Stratified split complete. Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}\")\n\n\n# --- Verification Step (Optional but Recommended) ---\n# Let's verify that the distributions are indeed similar.\nprint(\"\\n--- Verifying Score Distributions ---\")\noriginal_dist = pd.Series(y_train_all).value_counts(normalize=True).sort_index()\ntrain_targets = [full_dataset[i][3].item() for i in train_indices]\ntrain_dist = pd.Series(train_targets).value_counts(normalize=True).sort_index()\nval_targets = [full_dataset[i][3].item() for i in val_indices]\nval_dist = pd.Series(val_targets).value_counts(normalize=True).sort_index()\n\ndist_df = pd.DataFrame({\n    'Original %': original_dist * 100,\n    'Train Split %': train_dist * 100,\n    'Val Split %': val_dist * 100\n}).fillna(0).round(2)\n\nprint(dist_df)\n\n\n# --- Create DataLoaders ---\n# The rest of the process is the same\nBATCH_SIZE = 32\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nprint(f\"\\nDataLoaders created successfully!\")\nprint(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:02:25.670768Z","iopub.execute_input":"2025-11-17T22:02:25.670935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import DataLoader\nimport optuna\nfrom optuna.trial import TrialState\n\n# =====================================================================================\n# SECTION 1: MODEL DEFINITION\n# =====================================================================================\n# The HierarchicalAttentionScorer class remains the same as before.\nclass HierarchicalAttentionScorer(nn.Module):\n    def __init__(self, embed_dim=768, num_heads=8, dropout=0.1):\n        super().__init__()\n        self.prompt_response_attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n        self.metric_context_attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n        self.ln1 = nn.LayerNorm(embed_dim)\n        self.ln2 = nn.LayerNorm(embed_dim)\n        self.prediction_head = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim // 2), nn.ReLU(), nn.Dropout(dropout), nn.Linear(embed_dim // 2, 1)\n        )\n    def forward(self, metric_embedding, prompt_embedding, response_embedding):\n        prompt_embedding = prompt_embedding.unsqueeze(1)\n        response_embedding = response_embedding.unsqueeze(1)\n        metric_embedding = metric_embedding.unsqueeze(1)\n        contextual_prompt, _ = self.prompt_response_attention(query=prompt_embedding, key=response_embedding, value=response_embedding)\n        contextual_prompt = self.ln1(contextual_prompt + prompt_embedding)\n        final_representation, _ = self.metric_context_attention(query=metric_embedding, key=contextual_prompt, value=contextual_prompt)\n        final_representation = self.ln2(final_representation + metric_embedding)\n        return self.prediction_head(final_representation.squeeze(1))\n\n# =====================================================================================\n# SECTION 2: OPTUNA OBJECTIVE FUNCTION (NOW TUNES WEIGHTS)\n# =====================================================================================\n\ndef objective(trial):\n    # --- 1. Suggest Hyperparameters for Model and Optimizer ---\n    num_heads = trial.suggest_categorical('num_heads', [4, 8 ,16])\n    dropout = trial.suggest_float('dropout', 0.1, 0.4)\n    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-4, log=True)\n    \n    # --- 2. NEW: Suggest Hyperparameters for Loss Weights ---\n    # We suggest weights for each of our custom tiers.\n    # Using a log scale is good for searching over different orders of magnitude.\n    weight_rare = trial.suggest_float('weight_rare', 1000.0, 3000.0, log=True)\n    weight_mid_rare = trial.suggest_float('weight_mid_rare', 80.0, 500.0, log=True)\n    weight_8 = trial.suggest_float('weight_8', 10.0, 100.0, log=True)\n    weight_common =2.0 # Keep the common class weight fixed at 1.0 for a stable baseline\n    \n    # --- 3. Define the Weighted Loss Function for this specific trial ---\n    def calculate_loss_weights_trial(targets, device):\n        weights = torch.ones_like(targets).to(device)\n        mask_common = targets > 8.0\n        mask_8 = targets == 8.0\n        mask_mid_rare = (targets < 8.0) & (targets >= 6.0)\n        mask_rare = targets < 6.0\n        weights[mask_rare] = weight_rare\n        weights[mask_mid_rare] = weight_mid_rare\n        weights[mask_8] = weight_8\n        weights[mask_common] = weight_common\n        return weights\n\n    # --- 4. Initialize Model and Optimizer with Suggested Params ---\n    model = HierarchicalAttentionScorer(num_heads=num_heads, dropout=dropout).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    criterion = nn.MSELoss(reduction='none')\n\n    # --- 5. Training and Validation Loop ---\n    EPOCHS =100\n    best_val_rmse = float('inf')\n\n    for epoch in range(EPOCHS):\n        model.train()\n        for m_emb, p_emb, r_emb, target in train_loader:\n            m_emb, p_emb, r_emb, target = m_emb.to(device), p_emb.to(device), r_emb.to(device), target.to(device)\n            optimizer.zero_grad()\n            preds = model(m_emb, p_emb, r_emb)\n            loss_per_sample = criterion(preds, target)\n            # Use the loss function defined for this trial\n            weights = calculate_loss_weights_trial(target, device)\n            weighted_loss = (loss_per_sample * weights).mean()\n            weighted_loss.backward()\n            optimizer.step()\n            \n        model.eval()\n        weighted_val_mse_sum = 0.0\n        with torch.no_grad():\n            for m_emb, p_emb, r_emb, target in val_loader:\n                m_emb, p_emb, r_emb, target = m_emb.to(device), p_emb.to(device), r_emb.to(device), target.to(device)\n                preds = model(m_emb, p_emb, r_emb)\n                loss_per_sample = nn.MSELoss(reduction='none')(preds, target)\n                weights = calculate_loss_weights_trial(target, device)\n                weighted_val_mse_sum += (loss_per_sample * weights).sum().item()\n        \n        val_rmse = np.sqrt(weighted_val_mse_sum / len(val_dataset))\n\n        if val_rmse < best_val_rmse:\n            best_val_rmse = val_rmse\n            \n        trial.report(best_val_rmse, epoch)\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n\n    return best_val_rmse\n\n# =====================================================================================\n# SECTION 3: RUN OPTUNA STUDY\n# =====================================================================================\nprint(\"\\n--- Starting Optuna Hyperparameter and Loss Weight Search ---\")\nstudy = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner())\nstudy.optimize(objective, n_trials=150) # Increase n_trials for a more thorough search\n\nprint(\"\\n--- Optuna Search Complete ---\")\nbest_trial = study.best_trial\nprint(f\"  Value (Best Weighted Val RMSE): {best_trial.value}\")\nprint(\"  Best Params: \")\nfor key, value in best_trial.params.items():\n    print(f\"    {key}: {value:.6f}\")\n\n# =====================================================================================\n# SECTION 4: TRAIN FINAL MODEL WITH BEST PARAMS AND BEST WEIGHTS\n# =====================================================================================\nprint(\"\\n--- Training Final Model with Best Hyperparameters and Weights ---\")\n# --- 1. Get Best Params and Initialize Final Model ---\nbest_params = study.best_params\nfinal_model = HierarchicalAttentionScorer(\n    num_heads=best_params['num_heads'],\n    dropout=best_params['dropout']\n).to(device)\n\n# --- 2. Create the final, optimized weight function ---\ndef calculate_final_loss_weights(targets, device):\n    weights = torch.ones_like(targets).to(device)\n    mask_common = targets > 8.0\n    mask_8 = targets == 8.0\n    mask_mid_rare = (targets < 8.0) & (targets >= 6.0)\n    mask_rare = targets < 6.0\n    weights[mask_rare] = best_params['weight_rare']\n    weights[mask_mid_rare] = best_params['weight_mid_rare']\n    weights[mask_8] = best_params['weight_8']\n    weights[mask_common] = 1.0 # Still keep common class at 1.0\n    return weights\n\noptimizer = torch.optim.AdamW(final_model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])\ncriterion = nn.MSELoss(reduction='none')\nEPOCHS = 100\nbest_val_rmse = float('inf')\n\n# --- 3. Final Training Loop ---\nfor epoch in range(EPOCHS):\n    final_model.train()\n    train_loss = 0.0\n    for m_emb, p_emb, r_emb, target in train_loader:\n        m_emb, p_emb, r_emb, target = m_emb.to(device), p_emb.to(device), r_emb.to(device), target.to(device)\n        optimizer.zero_grad()\n        preds = final_model(m_emb, p_emb, r_emb)\n        loss_per_sample = criterion(preds, target)\n        # Use the FINAL optimized weight function\n        weights = calculate_final_loss_weights(target, device)\n        weighted_loss = (loss_per_sample * weights).mean()\n        weighted_loss.backward()\n        optimizer.step()\n        train_loss += weighted_loss.item()\n        \n    final_model.eval()\n    weighted_val_mse_sum = 0.0\n    with torch.no_grad():\n        for m_emb, p_emb, r_emb, target in val_loader:\n            m_emb, p_emb, r_emb, target = m_emb.to(device), p_emb.to(device), r_emb.to(device), target.to(device)\n            preds = final_model(m_emb, p_emb, r_emb)\n            loss_per_sample = nn.MSELoss(reduction='none')(preds, target)\n            weights = calculate_final_loss_weights(target, device)\n            weighted_val_mse_sum += (loss_per_sample * weights).sum().item()\n    val_rmse = np.sqrt(weighted_val_mse_sum / len(val_dataset))\n    \n    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss/len(train_loader):.4f} | Val Weighted RMSE: {val_rmse:.4f}\")\n    if val_rmse < best_val_rmse:\n        best_val_rmse = val_rmse\n        torch.save(final_model.state_dict(), \"best_attn_model_fully_tuned.pth\")\n        print(f\"   -> New best model saved with RMSE: {best_val_rmse:.4f}\")\n\n# --- 4. Inference & Submission ---\nprint(\"\\nGenerating Submission with Fully Tuned Model...\")\nfinal_model.load_state_dict(torch.load(\"best_attn_model_fully_tuned.pth\"))\nfinal_model.eval()\n# (Inference code is the same as before)\ntest_metric_tensors = torch.tensor(test_data_dict['metric_embedding'], dtype=torch.float32).to(device)\ntest_prompt_tensors = torch.tensor(test_data_dict['prompt_embedding'], dtype=torch.float32).to(device)\ntest_response_tensors = torch.tensor(test_data_dict['response_embedding'], dtype=torch.float32).to(device)\ntest_dataset_final = TensorDataset(test_metric_tensors, test_prompt_tensors, test_response_tensors)\ntest_loader_final = DataLoader(test_dataset_final, batch_size=64, shuffle=False)\nall_preds = []\nwith torch.no_grad():\n    for m_emb, p_emb, r_emb in test_loader_final:\n        preds = final_model(m_emb, p_emb, r_emb)\n        all_preds.extend(preds.cpu().numpy().flatten())\nfinal_preds = np.clip(all_preds, 0, 10)\ntest_df['ID'] = test_df.index + 1\nsubmission_df = pd.DataFrame({'ID': test_df['ID'], 'score': final_preds})\nsubmission_df.to_csv('submission_attn_regression_fully_tuned.csv', index=False)\nprint(\"Submission file 'submission_attn_regression_fully_tuned.csv' created successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# --- Common Setup Block ---\n\n# 1. Define the device for PyTorch\n# This is the most critical missing piece. The VQ-VAE code relies on this variable.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device set to: {device}\")\n\n# 2. Re-confirm necessary variables from the previous step are available.\n# The following variables were created in your previous code block and are needed by the VQ-VAE pipeline.\n# We'll just confirm they exist.\ntry:\n    _ = full_metric_tensors\n    _ = full_prompt_tensors\n    _ = full_response_tensors\n    _ = full_target_tensors\n    _ = test_data_dict\n    _ = y_train_all\n    print(\"Prerequisite tensors and data dictionaries are available.\")\nexcept NameError as e:\n    print(f\"CRITICAL ERROR: A required variable is missing: {e}\")\n    print(\"Please ensure the data preprocessing and splitting block has been run successfully before this step.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install scikit-learn==1.1.3 --quiet\n!pip install imbalanced-learn==0.10.1 --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTENC\nprint(\"SMOTENC imported successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import DataLoader, TensorDataset\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nimport gc\nfrom imblearn.over_sampling import SMOTENC\n\n# =====================================================================================\n# CONTROL PANEL & HYPERPARAMETERS\n# =====================================================================================\n# VQ-VAE Parameters\nNUM_EMBEDDINGS = 256# Size of the codebook (how many discrete codes to learn)\nEMBEDDING_DIM = 64    # Dimension of each code in the codebook\nCOMMITMENT_COST = 0.25\nVQVAE_EPOCHS = 10000     # How many epochs to train each VQ-VAE\nVQVAE_LR = 1e-5\n\n# XGBoost Parameters\nXGB_EPOCHS = 10000\nXGB_LR = 0.0005\n# =====================================================================================\n\n\n# =====================================================================================\n# SECTION 1: VQ-VAE MODEL DEFINITION\n# =====================================================================================\n\nclass VectorQuantizer(nn.Module):\n    \"\"\"The Vector Quantization Layer.\"\"\"\n    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n        super(VectorQuantizer, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.num_embeddings = num_embeddings\n        self.commitment_cost = commitment_cost\n        \n        # Initialize the codebook\n        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n        self.embedding.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)\n\n    def forward(self, inputs):\n        # Flatten input\n        flat_input = inputs.view(-1, self.embedding_dim)\n        \n        # Calculate distances\n        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n                    + torch.sum(self.embedding.weight**2, dim=1)\n                    - 2 * torch.matmul(flat_input, self.embedding.weight.t()))\n            \n        # Encoding\n        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings, device=inputs.device)\n        encodings.scatter_(1, encoding_indices, 1)\n        \n        # Quantize and unflatten\n        quantized = torch.matmul(encodings, self.embedding.weight).view(inputs.shape)\n        \n        # Calculate loss\n        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n        \n        # Straight-through estimator\n        quantized = inputs + (quantized - inputs).detach()\n        \n        return loss, quantized, encoding_indices.squeeze()\n\nclass VQVAE(nn.Module):\n    \"\"\"The full VQ-VAE model.\"\"\"\n    def __init__(self, input_dim=768, hidden_dim=256, embedding_dim=EMBEDDING_DIM, num_embeddings=NUM_EMBEDDINGS, commitment_cost=COMMITMENT_COST):\n        super(VQVAE, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, embedding_dim)\n        )\n        self.vq_layer = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)\n        self.decoder = nn.Sequential(\n            nn.Linear(embedding_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, input_dim)\n        )\n\n    def forward(self, x):\n        z = self.encoder(x)\n        vq_loss, quantized, indices = self.vq_layer(z)\n        x_recon = self.decoder(quantized)\n        return vq_loss, x_recon, indices\n\n# =====================================================================================\n# SECTION 2: VQ-VAE TRAINING AND LATENT CODE EXTRACTION\n# =====================================================================================\n\ndef train_vqvae(model, loader, optimizer, epochs=VQVAE_EPOCHS):\n    \"\"\"Generic training loop for a VQ-VAE model.\"\"\"\n    model.train()\n    for epoch in range(epochs):\n        for data in loader:\n            data = data[0].to(device)\n            optimizer.zero_grad()\n            vq_loss, data_recon, _ = model(data)\n            recon_error = F.mse_loss(data_recon, data)\n            loss = recon_error + vq_loss\n            loss.backward()\n            optimizer.step()\n    print(\"Training complete.\")\n\ndef extract_latent_codes(model, loader):\n    \"\"\"Extracts the discrete latent codes for a given dataset.\"\"\"\n    model.eval()\n    codes = []\n    with torch.no_grad():\n        for data in loader:\n            data = data[0].to(device)\n            _, _, indices = model(data)\n            codes.append(indices.cpu().numpy())\n    return np.concatenate(codes)\n\n# --- Prepare DataLoaders for each embedding type ---\n# We train the VQ-VAEs on the FULL training dataset (train+val)\nfull_train_dataset = TensorDataset(full_metric_tensors, full_prompt_tensors, full_response_tensors, full_target_tensors)\n\nmetric_loader_train = DataLoader(TensorDataset(full_metric_tensors), batch_size=128)\nprompt_loader_train = DataLoader(TensorDataset(full_prompt_tensors), batch_size=128)\nresponse_loader_train = DataLoader(TensorDataset(full_response_tensors), batch_size=128)\n\n# --- Train a VQ-VAE for each embedding type ---\nprint(\"\\n--- Training VQ-VAE for Metric Embeddings ---\")\nvqvae_metric = VQVAE().to(device)\noptimizer_metric = torch.optim.AdamW(vqvae_metric.parameters(), lr=VQVAE_LR)\ntrain_vqvae(vqvae_metric, metric_loader_train, optimizer_metric)\n\nprint(\"\\n--- Training VQ-VAE for Prompt Embeddings ---\")\nvqvae_prompt = VQVAE().to(device)\noptimizer_prompt = torch.optim.AdamW(vqvae_prompt.parameters(), lr=VQVAE_LR)\ntrain_vqvae(vqvae_prompt, prompt_loader_train, optimizer_prompt)\n\nprint(\"\\n--- Training VQ-VAE for Response Embeddings ---\")\nvqvae_response = VQVAE().to(device)\noptimizer_response = torch.optim.AdamW(vqvae_response.parameters(), lr=VQVAE_LR)\ntrain_vqvae(vqvae_response, response_loader_train, optimizer_response)\n\n# --- Extract Latent Codes to create the new dataset for XGBoost ---\nprint(\"\\n--- Extracting Latent Codes for XGBoost ---\")\n# For training data\nmetric_codes_train = extract_latent_codes(vqvae_metric, metric_loader_train)\nprompt_codes_train = extract_latent_codes(vqvae_prompt, prompt_loader_train)\nresponse_codes_train = extract_latent_codes(vqvae_response, response_loader_train)\n\n# For test data\ntest_metric_tensors = torch.tensor(test_data_dict['metric_embedding'], dtype=torch.float32)\ntest_prompt_tensors = torch.tensor(test_data_dict['prompt_embedding'], dtype=torch.float32)\ntest_response_tensors = torch.tensor(test_data_dict['response_embedding'], dtype=torch.float32)\n\nmetric_loader_test = DataLoader(TensorDataset(test_metric_tensors), batch_size=128)\nprompt_loader_test = DataLoader(TensorDataset(test_prompt_tensors), batch_size=128)\nresponse_loader_test = DataLoader(TensorDataset(test_response_tensors), batch_size=128)\n\nmetric_codes_test = extract_latent_codes(vqvae_metric, metric_loader_test)\nprompt_codes_test = extract_latent_codes(vqvae_prompt, prompt_loader_test)\nresponse_codes_test = extract_latent_codes(vqvae_response, response_loader_test)\n\n# --- Create the final DataFrame with latent codes as features ---\nX_train_xgb_df = pd.DataFrame({\n    'metric_code': metric_codes_train,\n    'prompt_code': prompt_codes_train,\n    'response_code': response_codes_train\n}).astype('category') # Treat codes as categorical features\n\nX_test_xgb_df = pd.DataFrame({\n    'metric_code': metric_codes_test,\n    'prompt_code': prompt_codes_test,\n    'response_code': response_codes_test\n}).astype('category')\n\ny_train_xgb = y_train_all\n\nprint(f\"XGBoost training features shape: {X_train_xgb_df.shape}\")\nprint(\"Sample of new features:\\n\", X_train_xgb_df.head())\n\n# =====================================================================================\n# SECTION 3: TIERED OVERSAMPLING WITH SMOTE-NC AND XGBOOST TRAINING\n# =====================================================================================\n\nprint(\"\\n--- Applying Tiered SMOTE-NC to the Training Data ---\")\n\n# --- 1. Prepare Data and Define the Oversampling Strategy ---\n# SMOTE-NC requires discrete class labels for its 'y' input.\n# We'll round the scores to the nearest integer to create these classes.\ny_train_classes = np.round(y_train_xgb).astype(int)\nclass_counts = pd.Series(y_train_classes).value_counts()\n\n# Define the desired number of samples for each class after oversampling.\n# This gives you precise control, as requested.\nsampling_strategy = {\n    # Scores > 8.0 (e.g., 9, 10) are not included, so they will not be oversampled.\n    # Scores == 8.0 get multiplied by 4\n    8: class_counts.get(8, 0) * 4,\n    # Scores == 7.0 get multiplied by 15\n    7: class_counts.get(7, 0) * 15,\n    # Scores == 6.0 get multiplied by 15\n    6: class_counts.get(6, 0) * 15\n}\n# Scores < 6.0 get multiplied by 80\nfor score in class_counts.index:\n    if score < 6:\n        sampling_strategy[score] = class_counts.get(score, 0) * 80\n\nprint(\"Original Class Distribution:\\n\", class_counts.sort_index())\nprint(\"\\nTarget Sampling Strategy for SMOTE-NC:\")\n# Sort the dictionary for printing\nsorted_strategy = {k: v for k, v in sorted(sampling_strategy.items())}\nfor k, v in sorted_strategy.items():\n    print(f\"  Score {k}: Target {v} samples\")\n\n# --- 2. Initialize and Apply SMOTE-NC ---\n# We must tell SMOTE-NC that all our features (columns 0, 1, 2) are categorical.\nsmote_nc = SMOTENC(\n    categorical_features=[0, 1, 2],\n    sampling_strategy=sampling_strategy,\n    random_state=42,\n    # Use a small k_neighbors value, as some classes have very few original samples.\n    # If a class has fewer than k_neighbors+1 samples, SMOTE-NC will fail.\n    # We find the smallest minority class count to set k_neighbors safely.\n    k_neighbors=min(2, min(class_counts[class_counts.index < 6]) - 1) if any(class_counts.index < 6) else 2\n)\n\nprint(f\"\\nUsing k_neighbors = {smote_nc.k_neighbors_}\")\nprint(f\"Original training data shape: {X_train_xgb_df.shape}\")\n\n# Apply the resampling. This can be memory-intensive.\nX_resampled, y_resampled_classes = smote_nc.fit_resample(X_train_xgb_df, y_train_classes)\n\n# The output 'y' is integer classes. For regression, we convert them back to float.\ny_resampled = y_resampled_classes.astype(float)\n\nprint(f\"Resampled training data shape: {X_resampled.shape}\")\nprint(\"\\nNew Score Distribution after Tiered SMOTE-NC:\")\nprint(pd.Series(y_resampled).value_counts().sort_index())\n\n# --- 3. Define XGBoost Parameters ---\nparams_xgb = {\n    'objective': 'reg:squarederror',\n    'eval_metric': 'rmse',\n    'n_estimators': XGB_EPOCHS,\n    'learning_rate': XGB_LR,\n    'max_depth': 6, # Can potentially use a slightly deeper tree with more data\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'seed': 42,\n    'n_jobs': -1,\n    'tree_method': 'gpu_hist' if 'cuda' in device.type else 'hist',\n    'enable_categorical': True  # CRUCIAL\n}\n\n# --- 4. Train the Final XGBoost Model on Resampled Data ---\nprint(\"\\n--- Training Final XGBoost Regressor on Resampled Latent Codes ---\")\nxgb_model_smote = xgb.XGBRegressor(**params_xgb)\n\n# IMPORTANT: We use the resampled data (X_resampled, y_resampled)\n# We DO NOT use sample_weight here, as the data has already been balanced by SMOTE-NC.\nxgb_model_smote.fit(X_resampled, y_resampled, verbose=False)\n\n# --- 5. Generate Predictions on the Original Test Set ---\nprint(\"\\nGenerating final predictions with the XGBoost model...\")\nfinal_preds_smote = xgb_model_smote.predict(X_test_xgb_df)\n\n# --- 6. Create Submission File ---\nfinal_preds_smote = np.clip(final_preds_smote, 0, 10)\ntest_df['ID'] = test_df.index + 1\nsubmission_df_smote = pd.DataFrame({'ID': test_df['ID'], 'score': final_preds_smote})\nsubmission_df_smote.to_csv('submission_vqvae_xgb_tiered_smotenc.csv', index=False)\nprint(\"Submission file 'submission_vqvae_xgb_tiered_smotenc.csv' created successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}